{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbcd305d-7ac6-41b2-bf50-f80a5ee4fd7e",
      "metadata": {
        "id": "dbcd305d-7ac6-41b2-bf50-f80a5ee4fd7e",
        "outputId": "05cb5c5f-d8b1-45f8-f65c-5a9f16d59626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_path = 'distilbert/distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c1fd97-6005-4e2f-927d-98e7a30ff8b2",
      "metadata": {
        "id": "89c1fd97-6005-4e2f-927d-98e7a30ff8b2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class BertMultiRegressor(nn.Module):\n",
        "    \"\"\"\n",
        "    Модель мульти регрессора на основе эмбеддингов BERT\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bert_model_name, output_size):\n",
        "        super(BertMultiRegressor, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
        "        self.regressor = nn.Linear(self.bert.config.hidden_size + 7, output_size)\n",
        "\n",
        "    def forward(self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        audio_traits=None,\n",
        "        labels=None,):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Объединение эмбеддингов и аудио вектора\n",
        "        result = torch.concat((last_hidden_state, audio_traits), dim=1)\n",
        "\n",
        "        outputs = self.regressor(result).to(torch.float64)\n",
        "\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "\n",
        "            loss_fn = nn.L1Loss()\n",
        "            losses = [loss_fn(outputs[i], labels[i]) for i in range(outputs.size()[0])]\n",
        "            loss = sum(losses)/len(losses)\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": outputs\n",
        "        }\n",
        "\n",
        "\n",
        "model = BertMultiRegressor(model_path, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6089d385-ef39-41f3-bb0b-a60eb54056c3",
      "metadata": {
        "id": "6089d385-ef39-41f3-bb0b-a60eb54056c3",
        "outputId": "89cf6507-e349-46e3-94a3-486fe6f6be00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertMultiRegressor(\n",
              "  (bert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (regressor): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda2299d-050c-4780-b18b-10642f89a425",
      "metadata": {
        "id": "eda2299d-050c-4780-b18b-10642f89a425"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Формирование датасета для обучения\n",
        "\n",
        "def process_dataset(data, data_trans, data_emo, data_cap, data_audio):\n",
        "    result_df = pd.DataFrame()\n",
        "    for emotion in data['emotion'].unique():\n",
        "        temp_df = data[data['emotion'] == emotion][['video_path', 'annotation']]\n",
        "        temp_df = temp_df.rename(columns={'annotation': emotion})\n",
        "\n",
        "        if result_df.empty:\n",
        "            result_df = temp_df\n",
        "        else:\n",
        "            result_df = result_df.merge(temp_df, on='video_path', how='outer')\n",
        "    final_df = result_df.merge(data_trans, on='video_path', how='outer')\n",
        "    data_cap = data_cap.rename(columns={'Filename': 'video_path'})\n",
        "    final_df = final_df.merge(data_cap, on='video_path', how='outer')\n",
        "    data_emo = data_emo.rename(columns={'Filename': 'video_path'})\n",
        "    final_df = final_df.merge(data_emo[['video_path', 'Emotion']], on='video_path', how='outer')\n",
        "    data_audio = data_audio.rename(columns={'Filename': 'video_path'})\n",
        "    final_df = final_df.merge(data_audio, on='video_path', how='outer')\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feaf2d3d-e47a-4d20-aad5-7f4ca637cccf",
      "metadata": {
        "id": "feaf2d3d-e47a-4d20-aad5-7f4ca637cccf",
        "outputId": "99114cb2-8f79-48ba-a862-8114eff29cff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_path</th>\n",
              "      <th>extraversion</th>\n",
              "      <th>neuroticism</th>\n",
              "      <th>agreeableness</th>\n",
              "      <th>conscientiousness</th>\n",
              "      <th>interview</th>\n",
              "      <th>openness</th>\n",
              "      <th>transcription</th>\n",
              "      <th>Description</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>AudioFeatures</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-6otZ7M-Mro.003.mp4</td>\n",
              "      <td>0.710280</td>\n",
              "      <td>0.552083</td>\n",
              "      <td>0.681319</td>\n",
              "      <td>0.728155</td>\n",
              "      <td>0.654206</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>... About you. I have a third nipple. It doesn...</td>\n",
              "      <td>a man with blonde hair and a plaid shirt</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[121.996249412252, 5191.308075129179, 18.4303,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-6otZ7M-Mro.005.mp4</td>\n",
              "      <td>0.523364</td>\n",
              "      <td>0.635417</td>\n",
              "      <td>0.626374</td>\n",
              "      <td>0.728155</td>\n",
              "      <td>0.644860</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>Beauty gurus, whatever you want to call them. ...</td>\n",
              "      <td>a man with blonde hair and a plaid shirt</td>\n",
              "      <td>angry</td>\n",
              "      <td>[115.36711503771465, 2358.436542021055, 23.853...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            video_path  extraversion  neuroticism  agreeableness  \\\n",
              "0  -6otZ7M-Mro.003.mp4      0.710280     0.552083       0.681319   \n",
              "1  -6otZ7M-Mro.005.mp4      0.523364     0.635417       0.626374   \n",
              "\n",
              "   conscientiousness  interview  openness  \\\n",
              "0           0.728155   0.654206  0.666667   \n",
              "1           0.728155   0.644860  0.600000   \n",
              "\n",
              "                                       transcription  \\\n",
              "0  ... About you. I have a third nipple. It doesn...   \n",
              "1  Beauty gurus, whatever you want to call them. ...   \n",
              "\n",
              "                                Description  Emotion  \\\n",
              "0  a man with blonde hair and a plaid shirt  neutral   \n",
              "1  a man with blonde hair and a plaid shirt    angry   \n",
              "\n",
              "                                       AudioFeatures  \n",
              "0  [121.996249412252, 5191.308075129179, 18.4303,...  \n",
              "1  [115.36711503771465, 2358.436542021055, 23.853...  "
            ]
          },
          "execution_count": 234,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_texts = process_dataset(pd.read_csv('annotation_validation.csv'), pd.read_csv('transcription_validation.csv'),\n",
        "                              pd.read_csv('emotions_validation.csv'), pd.read_csv('video_captions_validation.csv'),\n",
        "                              pd.read_csv('audio_features_validation.csv'))\n",
        "test_texts.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d651d092-1d30-420f-a361-c281dd1fd93c",
      "metadata": {
        "id": "d651d092-1d30-420f-a361-c281dd1fd93c",
        "outputId": "cb167db1-1e6d-4b69-e2cb-5bea0074e607"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = process_dataset(pd.read_csv('annotation_training.csv'), pd.read_csv('transcription_training.csv'),\n",
        "                                pd.read_csv('emotions_train.csv'), pd.read_csv('video_captions.csv'),\n",
        "                                pd.read_csv('audio_features_train.csv'))\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7bb3a5-6eaf-4e97-8e9c-0ab97d56348f",
      "metadata": {
        "id": "ad7bb3a5-6eaf-4e97-8e9c-0ab97d56348f"
      },
      "outputs": [],
      "source": [
        "test_dataset = process_dataset(pd.read_csv('annotation_validation.csv'), pd.read_csv('transcription_validation.csv'),\n",
        "                              pd.read_csv('emotions_validation.csv'), pd.read_csv('video_captions_validation.csv'),\n",
        "                              pd.read_csv('audio_features_validation.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89462d9-e972-4130-83a0-20ddf0b9dedd",
      "metadata": {
        "id": "b89462d9-e972-4130-83a0-20ddf0b9dedd",
        "outputId": "fdcb334f-43e6-4bcb-84fd-7a56599a353e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5766"
            ]
          },
          "execution_count": 238,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = train_dataset.dropna()\n",
        "train_dataset.reset_index(drop=True, inplace=True)\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "843cffc2-68af-48b2-8e2a-61f1087fd334",
      "metadata": {
        "id": "843cffc2-68af-48b2-8e2a-61f1087fd334",
        "outputId": "2e5ff769-7d42-4434-dcde-bcaefb719148"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1923"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset = test_dataset.dropna()\n",
        "test_dataset.reset_index(drop=True, inplace=True)\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510c7757-c92a-4e36-a532-a8d31d9d6d9e",
      "metadata": {
        "id": "510c7757-c92a-4e36-a532-a8d31d9d6d9e",
        "outputId": "14de7b0b-d4d2-4704-f4c4-cc804aeb433e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_path</th>\n",
              "      <th>extraversion</th>\n",
              "      <th>neuroticism</th>\n",
              "      <th>agreeableness</th>\n",
              "      <th>conscientiousness</th>\n",
              "      <th>interview</th>\n",
              "      <th>openness</th>\n",
              "      <th>transcription</th>\n",
              "      <th>Description</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>AudioFeatures</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>--Ymqszjv54.001.mp4</td>\n",
              "      <td>0.551402</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.527473</td>\n",
              "      <td>0.650485</td>\n",
              "      <td>0.588785</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>I like Tabasco sauce. I like Louisiana Hot Sau...</td>\n",
              "      <td>a man holding a bowl of vegetables in his hand</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[122.1400763651526, 680.116572306553, 49.11913...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>--Ymqszjv54.003.mp4</td>\n",
              "      <td>0.392523</td>\n",
              "      <td>0.427083</td>\n",
              "      <td>0.516484</td>\n",
              "      <td>0.475728</td>\n",
              "      <td>0.392523</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>Lot more things there. Then the menus are a lo...</td>\n",
              "      <td>a man in an orange hoodie is sitting in front ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[121.88748153289707, 1014.9409413938398, 53.44...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            video_path  extraversion  neuroticism  agreeableness  \\\n",
              "0  --Ymqszjv54.001.mp4      0.551402     0.500000       0.527473   \n",
              "1  --Ymqszjv54.003.mp4      0.392523     0.427083       0.516484   \n",
              "\n",
              "   conscientiousness  interview  openness  \\\n",
              "0           0.650485   0.588785  0.744444   \n",
              "1           0.475728   0.392523  0.466667   \n",
              "\n",
              "                                       transcription  \\\n",
              "0  I like Tabasco sauce. I like Louisiana Hot Sau...   \n",
              "1  Lot more things there. Then the menus are a lo...   \n",
              "\n",
              "                                         Description  Emotion  \\\n",
              "0     a man holding a bowl of vegetables in his hand  neutral   \n",
              "1  a man in an orange hoodie is sitting in front ...  neutral   \n",
              "\n",
              "                                       AudioFeatures  \n",
              "0  [122.1400763651526, 680.116572306553, 49.11913...  \n",
              "1  [121.88748153289707, 1014.9409413938398, 53.44...  "
            ]
          },
          "execution_count": 240,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b034c6c-37f0-496d-a1ec-d11a35b38421",
      "metadata": {
        "id": "0b034c6c-37f0-496d-a1ec-d11a35b38421"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "def preprocess_function(example):\n",
        "   # Объединение всех текстовых метрик в один вход\n",
        "   text = f\"There is {example['Description']}. This person feels {example['Emotion']}. This person says: {example['transcription']}\"\n",
        "   res = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "   res['labels'] = np.array(example[['extraversion', 'neuroticism', 'agreeableness',\n",
        "                                    'conscientiousness', 'interview', 'openness']].tolist())\n",
        "\n",
        "   res['audio_traits'] = torch.FloatTensor(literal_eval(example['AudioFeatures']))\n",
        "   return res\n",
        "\n",
        "train_dataset = train_dataset.apply(preprocess_function, axis=1)\n",
        "test_dataset = test_dataset.apply(preprocess_function, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec2e5c4-a004-4c1a-bf47-4fbfb6237dbc",
      "metadata": {
        "id": "fec2e5c4-a004-4c1a-bf47-4fbfb6237dbc",
        "outputId": "7b9eaedf-7bc7-492a-d259-4c00de918bf8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user1/environments/hack/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9610' max='9610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9610/9610 09:36, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Multirmse</th>\n",
              "      <th>Multimae</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.088700</td>\n",
              "      <td>0.147107</td>\n",
              "      <td>0.304377</td>\n",
              "      <td>0.147107</td>\n",
              "      <td>0.852893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.080900</td>\n",
              "      <td>0.120802</td>\n",
              "      <td>0.164951</td>\n",
              "      <td>0.120802</td>\n",
              "      <td>0.879198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.118761</td>\n",
              "      <td>0.187752</td>\n",
              "      <td>0.118761</td>\n",
              "      <td>0.881239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.061100</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.177369</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.881868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.052300</td>\n",
              "      <td>0.112364</td>\n",
              "      <td>0.150269</td>\n",
              "      <td>0.112364</td>\n",
              "      <td>0.887636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9610, training_loss=0.07286596011896136, metrics={'train_runtime': 576.3636, 'train_samples_per_second': 50.021, 'train_steps_per_second': 16.674, 'total_flos': 0.0, 'train_loss': 0.07286596011896136, 'epoch': 5.0})"
            ]
          },
          "execution_count": 258,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from torch import nn\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Запуск обучения\n",
        "\n",
        "def compute_metrics(p):\n",
        "    rmse_per_output = np.mean(np.sqrt(np.mean((p.label_ids - p.predictions) ** 2, axis=0)))\n",
        "    rmae_per_output = np.mean(np.abs(p.label_ids  - p.predictions))\n",
        "    return {'MultiRMSE': rmse_per_output,\n",
        "           'MultiMAE': rmae_per_output,\n",
        "           'Accuracy': 1 - rmae_per_output}\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"disbert_audio\",\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=3,\n",
        "  per_device_eval_batch_size=3,\n",
        "  num_train_epochs=5,\n",
        "  weight_decay=0.01,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train_dataset,\n",
        "   eval_dataset=test_dataset,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e801078f-425f-43e0-9950-b73c09fa9e8b",
      "metadata": {
        "id": "e801078f-425f-43e0-9950-b73c09fa9e8b",
        "outputId": "08e89314-63c0-4869-8195-fa1389b08f53"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='641' max='641' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [641/641 00:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.11236380481704547,\n",
              " 'eval_MultiRMSE': 0.15026899889002251,\n",
              " 'eval_MultiMAE': 0.11236380481704548,\n",
              " 'eval_Accuracy': 0.8876361951829546,\n",
              " 'eval_runtime': 9.5509,\n",
              " 'eval_samples_per_second': 201.343,\n",
              " 'eval_steps_per_second': 67.114,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 259,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52afaa0e-2c73-454d-89ab-d9c7d7046755",
      "metadata": {
        "id": "52afaa0e-2c73-454d-89ab-d9c7d7046755"
      },
      "outputs": [],
      "source": [
        "# Сохранение модели\n",
        "\n",
        "torch.save(model.regressor.state_dict(), 'reg_state_dict.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31ff67c-a49c-445a-be53-76ddcbcd64d0",
      "metadata": {
        "id": "b31ff67c-a49c-445a-be53-76ddcbcd64d0",
        "outputId": "444d26c5-e6fa-4de7-f81e-bc3a1ec161c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('models/hope_bert/tokenizer_config.json',\n",
              " 'models/hope_bert/special_tokens_map.json',\n",
              " 'models/hope_bert/vocab.txt',\n",
              " 'models/hope_bert/added_tokens.json',\n",
              " 'models/hope_bert/tokenizer.json')"
            ]
          },
          "execution_count": 261,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path = 'models/regressor/'\n",
        "model.bert.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}